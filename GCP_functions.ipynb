{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función de ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion para cloud function\n",
    "\n",
    "gcloud functions deploy nombre_de_la_funcion \\\n",
    "    --runtime python310 \\\n",
    "    --trigger-resource nombre_del_bucket \\\n",
    "    --trigger-event google.storage.object.finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "def cargar_a_bigquery(project_id, dataset_id, table_id, gcs_bucket, gcs_object):\n",
    "    # Inicializa el cliente de BigQuery y Cloud Storage\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    gcs_client = storage.Client(project=project_id)\n",
    "\n",
    "    # Define la ubicación del archivo en Cloud Storage\n",
    "    gcs_uri = f'gs://{gcs_bucket}/{gcs_object}'\n",
    "\n",
    "    # Configura la referencia a la tabla de BigQuery\n",
    "    table_ref = bq_client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "    # Configura las opciones de carga\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.json,  # Ajusta según el formato de tu archivo\n",
    "        skip_leading_rows=0,  # Si hay encabezados en el archivo, ajusta esta opción\n",
    "        autodetect=True,  # Deja que BigQuery detecte automáticamente el esquema\n",
    "    )\n",
    "\n",
    "    # Inicia el trabajo de carga\n",
    "    load_job = bq_client.load_table_from_uri(\n",
    "        gcs_uri, table_ref, job_config=job_config\n",
    "    )\n",
    "\n",
    "    # Espera a que el trabajo de carga se complete\n",
    "    load_job.result()\n",
    "\n",
    "    print(f'Carga completada en la tabla {project_id}.{dataset_id}.{table_id}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configura tus valores específicos\n",
    "    project_id = 'tu-proyecto'\n",
    "    dataset_id = 'tu-dataset'\n",
    "    table_id = 'tu-tabla'\n",
    "    gcs_bucket = 'tu-bucket'\n",
    "    gcs_object = 'ruta/del/archivo.csv'  # Ajusta la ruta del archivo en Cloud Storage\n",
    "\n",
    "    cargar_a_bigquery(project_id, dataset_id, table_id, gcs_bucket, gcs_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_a_bigquery(event, context):\n",
    "    \"\"\"Esta función se ejecuta cuando se detecta un nuevo archivo en el bucket.\"\"\"\n",
    "    bucket_name = event['bucket']\n",
    "    file_name = event['name']\n",
    "\n",
    "    # Configurar el cliente de BigQuery y Cloud Storage\n",
    "    bq_client = bigquery.Client()\n",
    "    gcs_client = storage.Client()\n",
    "\n",
    "    # Especificar el dataset y la tabla de destino en BigQuery\n",
    "    dataset_id = 'tu_dataset'\n",
    "    table_id = 'tu_tabla'\n",
    "\n",
    "    # Crear el URI del archivo en GCS\n",
    "    file_uri = f'gs://{bucket_name}/{file_name}'\n",
    "\n",
    "    # Configurar la carga del archivo a BigQuery\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[...],  # Especifica el esquema de tu tabla\n",
    "        source_format=bigquery.SourceFormat.CSV,  # O el formato de tu archivo\n",
    "    )\n",
    "\n",
    "    # Iniciar la carga del archivo\n",
    "    load_job = bq_client.load_table_from_uri(file_uri, dataset_id, table_id, job_config=job_config)\n",
    "    load_job.result()  # Esperar a que se complete la carga\n",
    "\n",
    "    print(f'Archivo {file_uri} cargado exitosamente a BigQuery.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "\n",
    "def cargar_a_bigquery(event, context):\n",
    "    \"\"\"Esta función se ejecuta cuando se detecta un nuevo archivo en el bucket.\"\"\"\n",
    "    bucket_name = event['bucket']\n",
    "    file_name = event['name']\n",
    "\n",
    "    # Configurar el cliente de BigQuery y Cloud Storage\n",
    "    bq_client = bigquery.Client()\n",
    "    gcs_client = storage.Client()\n",
    "\n",
    "    # Especificar el dataset y la tabla de destino en BigQuery\n",
    "    dataset_id = 'tu_dataset'\n",
    "    table_id = 'tu_tabla'\n",
    "\n",
    "    # Crear el URI del archivo en GCS\n",
    "    file_uri = f'gs://{bucket_name}/{file_name}'\n",
    "\n",
    "    # Configurar la carga del archivo a BigQuery\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[...],  # Especifica el esquema de tu tabla\n",
    "        source_format=bigquery.SourceFormat.CSV,  # O el formato de tu archivo\n",
    "    )\n",
    "\n",
    "    # Descargar el archivo de Cloud Storage y cargarlo en un DataFrame de pandas\n",
    "    bucket = gcs_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    data = blob.download_as_text()\n",
    "    df = pd.read_csv(pd.compat.StringIO(data))\n",
    "\n",
    "    # Realizar transformaciones en el DataFrame (por ejemplo, eliminar duplicados)\n",
    "    df_sin_duplicados = df.drop_duplicates()\n",
    "\n",
    "    # Guardar el DataFrame modificado en un nuevo archivo CSV en Cloud Storage\n",
    "    nuevo_file_name = f'processed/{file_name}'  # Cambia el nombre según tus necesidades\n",
    "    nuevo_blob = bucket.blob(nuevo_file_name)\n",
    "    nuevo_blob.upload_from_string(df_sin_duplicados.to_csv(index=False), content_type='text/csv')\n",
    "\n",
    "    print(f'Transformación completada. Archivo procesado guardado en {nuevo_blob}')\n",
    "\n",
    "    # Actualizar el URI del archivo con la ruta al archivo procesado\n",
    "    file_uri_procesado = f'gs://{bucket_name}/{nuevo_file_name}'\n",
    "\n",
    "    # Iniciar la carga del archivo procesado a BigQuery\n",
    "    load_job = bq_client.load_table_from_uri(file_uri_procesado, dataset_id, table_id, job_config=job_config)\n",
    "    load_job.result()  # Esperar a que se complete la carga\n",
    "\n",
    "    print(f'Archivo {file_uri_procesado} cargado exitosamente a BigQuery.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "def cargar_a_bigquery(event, context):\n",
    "\n",
    "    # Configurar el cliente de BigQuery y Cloud Storage\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Especificar el dataset y la tabla de destino en BigQuery\n",
    "    dataset_id = 'tu_dataset'\n",
    "    table_id = 'tu_tabla'\n",
    "\n",
    "    # Crear el URI del archivo en GCS\n",
    "    file_uri = f'gs://{bucket_name}/{file_name}'\n",
    "\n",
    "    # Configurar la carga del archivo a BigQuery\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[...],  # Especifica el esquema de tu tabla\n",
    "        source_format=bigquery.SourceFormat.CSV,  # O el formato de tu archivo\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Sobrescribir la tabla existente\n",
    "    )\n",
    "\n",
    "    # Iniciar la carga del archivo a BigQuery\n",
    "    load_job = bq_client.load_table_from_uri(file_uri, dataset_id, table_id, job_config=job_config)\n",
    "    load_job.result()  # Esperar a que se complete la carga\n",
    "\n",
    "    print(f'Archivo {file_uri} cargado exitosamente a BigQuery.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
